{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.1 Regular Expressions\n",
    "# It s like a language (not only programming one) rules...\n",
    "# They are a language unto themselves...\n",
    "# to use u need to import the library \"import re\"\n",
    "import re\n",
    "\n",
    "# to search:\n",
    "re.search() \n",
    "\n",
    "# to extract portions...\n",
    "re.findall()\n",
    "\n",
    "# u can do equivalents with regular expressions like:\n",
    "\n",
    "for line in lines:\n",
    "if line.starstwith('From'):\n",
    "# is equal:\n",
    "if re.search('^From', line):\n",
    "    \n",
    "# other example: using Wild-Card Characters\n",
    "# '.' = any character\n",
    "# '*' = any number of times\n",
    "# if u search for: '^A.*:'\n",
    "# you will be searching for lines that starts with a captalized 'a', followed by any characters and ending with a ':'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 Extracting Data\n",
    "# [0-9]+   --> is used to find numbers from 0 to 9 with 1 or more characters (+)...\n",
    "# using findal with the command above, bring us all the numbers in a sequence!\n",
    "# ex.: u can use the following expression to find emails...\n",
    "\n",
    "email = re.findal('\\S+@\\S+', filename)\n",
    "\n",
    "# '\\S+' menas \"at least one non-whitespace character\" so u got sequences that matches a email cause of the \"@\"\n",
    "\n",
    "# \"Greedyness\" is the fact that regular expressions bring the bigger version of your solicitation...\n",
    "# u can limit \"greedyness\" using \"?\"\n",
    "# u can limit the result using \"()\" like:\n",
    "email = re.findal('^From (\\S+@\\S+)', file)\n",
    "# this command will bring the '\\S+@\\S+' that matches the email from lines that starts with 'From' without bringing the From tiself!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.2 The Double Split Pattern\n",
    "sufixemail = re.findall('@([^ ]*)', file)\n",
    "# this one cuts the string in the @, them get all after that is not a blank character!\n",
    "\n",
    "### REGULAR EXPRESSIONS are a way of making code succinct, but can take u more time to read it...\n",
    "# dont over use it and make sure to comment it... but try to understand it and be aware that u can use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339528\n"
     ]
    }
   ],
   "source": [
    "'''Finding Numbers in a Haystack\n",
    "\n",
    "In this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers.\n",
    "\n",
    "Data Files\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/regex_sum_42.txt (There are 90 values with a sum=445833)\n",
    "Actual data: http://py4e-data.dr-chuck.net/regex_sum_1920403.txt (There are 77 values and the sum ends with 528)\n",
    "These links open in a new window. Make sure to save the file into the same folder as you will be writing your Python program. Note: Each student will have a distinct data file for the assignment - so only use your own data file for analysis.\n",
    "Data Format\n",
    "The file contains much of the text from the introduction of the textbook except that random numbers are inserted throughout the text. Here is a sample of the output you might see:'''\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "openfile = open('regex_sum_1920403.txt')\n",
    "numbers = list()\n",
    "sum = 0\n",
    "for lines in openfile:\n",
    "    lines = lines.rstrip()\n",
    "    numbers = re.findall('([0-9]+)', lines)\n",
    "    for n in numbers:\n",
    "        try:\n",
    "            n = int(n)\n",
    "            sum = sum + n\n",
    "        except:\n",
    "            continue\n",
    "print(sum)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.1 Networked Technology\n",
    "# Socket: connection between two aplications!\n",
    "# They use TCP Ports\n",
    "\n",
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect( ('data.pr4e.org', 80))  # here is the Host and the port number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.2 Networked Technology\n",
    "# Application Protocol\n",
    "# HTTP = Hypertext Transfer Protocol\n",
    "# This is the dominant Application Layer Protocol on the internet\n",
    "\n",
    "import socket\n",
    "mysock = socket.socket(socket.AF_ITNET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n' .encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "# this commands prepare the door to connection, then send a command to get data!\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()\n",
    "\n",
    "# this loop sets the command to get up to 512 characters of data to continue until a data of zero characters be gotten, then break and close the connection\n",
    "\n",
    "#12 Using the Developer Console to Explore HTTP\n",
    "# You can use the Developer COnsole in internet browsers to explore the HTTP infos\n",
    "# Status on Headers: \n",
    "# 200 - ok, file founded\n",
    "# 404 - no good, file not founded\n",
    "# 302 - no found, but get u the redirect to the file u want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 28 Oct 2023 11:54:47 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "#Exploring the HyperText Transport Protocol\n",
    "#\n",
    "# You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "# \n",
    "# http://data.pr4e.org/intro-short.txt\n",
    "# There are three ways that you might retrieve this web page and look at the response headers:\n",
    "#\n",
    "# Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "# Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n",
    "# Enter the header values in each of the fields below and press \"Submit\".\n",
    "\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# 12.3 - Unicode Characters and Strings\n",
    "# there are a code that converts letters in numbers! ASCII (American Standard Code for Information Interchange)\n",
    "# u can see the \"real\" number using 'ord()'\n",
    "\n",
    "print(ord('G'))\n",
    "\n",
    "# there are difficults to connect different languages computers to talk... image US and japanese characters!\n",
    "# UTF-8 was created to represent multi-byte Characters 1-4 bytes and got compatible with ASCII\n",
    "\n",
    "#.encode() # this puts the strings into bytes!!!\n",
    "# .decode() # figure out what kind of data u are getting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "#12.4 Retrieving Web Pages\n",
    "# Uring urllib in Python\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "filehandle = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in filehandle:\n",
    "    print(line.decode().strip())    \n",
    "    \n",
    "# following links:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (532249982.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install beautifulsoup4\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 12.5 - Parsing Web Pages\n",
    "# Web Scraping? --> get information from web sites\n",
    "# Take care about coryrights and ethic concerns...\n",
    "\n",
    "# The easy way The BeautifulSoup - install it \n",
    "\n",
    "pip install beautifulsoup4 # this must run in a box without comments to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\carlos\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\carlos\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.coursera.org/\n",
      "https://www.coursera.org/business?utm_content=corp-to-home-for-enterprise&utm_campaign=website&utm_medium=coursera&utm_source=header&utm_term=b-out\n",
      "https://www.coursera.org/campus?utm_content=corp-to-landing-for-campus&utm_campaign=website&utm_medium=coursera&utm_source=header&utm_term=b-out\n",
      "https://www.coursera.org/government?utm_content=corp-to-landing-for-government&utm_campaign=website&utm_medium=coursera&utm_source=header&utm_term=b-out\n",
      "/\n",
      "/degrees\n",
      "/mastertrack\n",
      "/certificates/learn\n",
      "/career-academy/?trk_ref=globalnav\n",
      "/browse\n",
      "/courses\n",
      "/lecture/python-network-data/12-5-parsing-web-pages-1oHBS?authMode=login\n",
      "/lecture/python-network-data/12-5-parsing-web-pages-1oHBS?authMode=signup\n",
      "/\n",
      "/learn/python-network-data\n",
      "/learn/python-network-data\n",
      "/learn/python-network-data\n",
      "/learn/python-network-data\n",
      "/specializations/python\n",
      "?authMode=signup\n",
      "/learn/python-network-data#syllabus\n",
      "/lecture/python-network-data/12-3-unicode-characters-and-strings-MbRIS\n",
      "/lecture/python-network-data/12-4-retrieving-web-pages-bwvyb\n",
      "/lecture/python-network-data/worked-example-using-urllib-chapter-12-kWTYV\n",
      "/lecture/python-network-data/12-5-parsing-web-pages-1oHBS\n",
      "/lecture/python-network-data/worked-example-beautifulsoup-chapter-12-S4FIR\n",
      "?authMode=signup\n",
      "https://www.coursera.org/collections/new-courses-on-coursera\n",
      "https://www.coursera.org/courses?query=free\n",
      "https://www.coursera.org/google-career-certificates\n",
      "https://www.coursera.org/courses?query=artificial%20intelligence\n",
      "https://www.coursera.org/courses?query=cybersecurity\n",
      "https://www.coursera.org/browse/data-science\n",
      "https://www.coursera.org/courses?query=microsoft%20excel\n",
      "https://www.coursera.org/learn/generative-ai-with-llms\n",
      "https://www.coursera.org/learn/prompt-engineering\n",
      "https://www.coursera.org/courses?query=python\n",
      "https://www.coursera.org/career-academy\n",
      "https://www.coursera.org/browse/computer-science\n",
      "https://www.coursera.org/browse/data-science/data-analysis\n",
      "https://www.coursera.org/courses?query=digital%20marketing\n",
      "https://www.coursera.org/browse/information-technology\n",
      "https://www.coursera.org/browse/language-learning/learning-english\n",
      "https://www.coursera.org/browse/data-science/machine-learning\n",
      "https://www.coursera.org/courses?query=power%20bi\n",
      "https://www.coursera.org/courses?query=product%20management\n",
      "https://www.coursera.org/courses?query=project%20management\n",
      "https://www.coursera.org/courses?query=web development\n",
      "https://www.coursera.org/courses\n",
      "https://www.coursera.org/professional-certificates/google-cybersecurity\n",
      "https://www.coursera.org/professional-certificates/google-data-analytics\n",
      "https://www.coursera.org/professional-certificates/google-digital-marketing-ecommerce\n",
      "https://www.coursera.org/professional-certificates/google-it-support\n",
      "https://www.coursera.org/professional-certificates/google-project-management\n",
      "https://www.coursera.org/professional-certificates/ibm-data-analyst\n",
      "https://www.coursera.org/professional-certificates/ibm-data-science\n",
      "https://www.coursera.org/professional-certificates/meta-front-end-developer\n",
      "https://www.coursera.org/professional-certificates/microsoft-power-bi-data-analyst\n",
      "https://www.coursera.org/certificates\n",
      "https://www.coursera.org/articles/strengths-and-weaknesses-interview\n",
      "https://www.coursera.org/articles/job-application-email\n",
      "https://www.coursera.org/articles/tutorial-highlight-duplicates-google-sheets\n",
      "https://www.coursera.org/articles/how-to-write-a-letter-of-recommendation-template-tips\n",
      "https://www.coursera.org/articles/popular-cybersecurity-certifications\n",
      "https://www.coursera.org/articles/high-income-skills\n",
      "https://www.coursera.org/articles/announce-new-job-linkedin\n",
      "https://www.coursera.org/articles/how-to-ask-for-a-letter-of-recommendation-template-tips\n",
      "https://www.coursera.org/articles/what-is-a-data-scientist\n",
      "https://www.coursera.org/articles/job-search-tips\n",
      "https://www.coursera.org/articles\n",
      "https://about.coursera.org/\n",
      "https://about.coursera.org/how-coursera-works/\n",
      "https://about.coursera.org/leadership\n",
      "https://careers.coursera.com/\n",
      "/browse\n",
      "/courseraplus\n",
      "/professional-certificate\n",
      "/mastertrack\n",
      "/degrees\n",
      "/business?utm_campaign=website&utm_content=corp-to-home-footer-for-enterprise&utm_medium=coursera&utm_source=enterprise\n",
      "/government?utm_campaign=website&utm_content=corp-to-home-footer-for-government&utm_medium=coursera&utm_source=enterprise\n",
      "/campus?utm_campaign=website&utm_content=corp-to-home-footer-for-campus&utm_medium=coursera&utm_source=enterprise\n",
      "https://partnerships.coursera.org/?utm_medium=coursera&utm_source=partnerships&utm_campaign=website&utm_content=corp-to-home-footer-become-a-partner\n",
      "https://www.coursera.support/s/article/360041137871-A-community-response-to-COVID-19\n",
      "/social-impact\n",
      "https://www.coursera.org/courses?query=free\n",
      "https://www.coursera.org/courses\n",
      "https://www.coursera.community\n",
      "https://www.coursera.org/about/partners\n",
      "https://www.coursera.support/s/article/360000152926-Become-a-Coursera-beta-tester\n",
      "https://translate-coursera.org\n",
      "https://blog.coursera.org\n",
      "https://medium.com/coursera-engineering\n",
      "/teaching-center\n",
      "/about/press\n",
      "https://investor.coursera.com\n",
      "/about/terms\n",
      "/about/privacy\n",
      "https://learner.coursera.help/hc\n",
      "https://learner.coursera.help/hc/articles/360050668591-Accessibility-Statement\n",
      "/about/contact\n",
      "https://www.coursera.org/articles\n",
      "/directory\n",
      "https://about.coursera.org/affiliates\n",
      "https://coursera_assets.s3.amazonaws.com/footer/Modern+Slavery+Statement+26+April+2023.pdf\n",
      "/about/cookies-manage\n",
      "https://itunes.apple.com/app/apple-store/id736535961?pt=2334150&ct=Coursera%20Web%20Promo%20Banner&mt=8\n",
      "http://play.google.com/store/apps/details?id=org.coursera.android\n",
      "https://www.facebook.com/Coursera\n",
      "https://www.linkedin.com/company/coursera\n",
      "https://twitter.com/coursera\n",
      "https://www.youtube.com/user/coursera\n",
      "https://www.instagram.com/coursera/\n",
      "https://www.tiktok.com/@coursera\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# retrieve all the anchor tags from the page\n",
    "tags = soup('a')    # find the '<a' in the html page code\n",
    "for tag in tags:         # this loop finds the 'href' that is the actually link\n",
    "    print(tag.get('href', None))\n",
    "    \n",
    "#runned with 'https://www.coursera.org/learn/python-network-data/lecture/1oHBS/12-5-parsing-web-pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2844\n"
     ]
    }
   ],
   "source": [
    "# Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "# We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "# Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "# Actual data: http://py4e-data.dr-chuck.net/comments_1920405.html (Sum ends with 44)\n",
    "# You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "# Data Format\n",
    "# The file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n",
    "\n",
    "# <tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n",
    "# <tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n",
    "# <tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n",
    "# You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n",
    "# Look at the sample code provided. It shows how to find all of a certain kind of tag, loop through the tags and extract the various aspects of the tags.\n",
    "\n",
    "# ...\n",
    "# Retrieve all of the anchor tags\n",
    "# tags = soup('a')\n",
    "# for tag in tags:\n",
    "#    # Look at the parts of a tag\n",
    "#    print 'TAG:',tag\n",
    "#    print 'URL:',tag.get('href', None)\n",
    "#    print 'Contents:',tag.contents[0]\n",
    "#    print 'Attrs:',tag.attrs\n",
    "# You need to adjust this code to look for span tags and pull out the text content of the span tag, convert them to integers and add them up to complete the assignment.\n",
    "# Sample Execution\n",
    "# \n",
    "# $ python3 solution.py\n",
    "# Enter - http://py4e-data.dr-chuck.net/comments_42.html\n",
    "# Count 50\n",
    "# Sum 2...\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url =  'http://py4e-data.dr-chuck.net/comments_1920405.html'\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "sum = 0\n",
    "tags = soup('span')    # find the '<span>' in the html page code\n",
    "for tag in tags:         # this loop sets each <span> in one line... \n",
    "    for n in tag.contents:     # then this other loop takes the numbers out of it\n",
    "        sum = sum + int(n)         # this sum converts the numbers in integers and sum it to the sum variable\n",
    "print(sum)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Sukhman.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Spencer.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Magdalene.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Richey.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Yuanyu.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Aimiee.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Eisa.html\n"
     ]
    }
   ],
   "source": [
    "# Following Links in Python\n",
    "\n",
    "# In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "# We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "# Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "# Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "# Last name in sequence: Anayah\n",
    "# Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Murdo.html\n",
    "# Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "# Hint: The first character of the name of the last page that you will load is: E\n",
    "# Strategy\n",
    "# The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\n",
    "# \n",
    "# Sample execution\n",
    "# \n",
    "# Here is a sample execution of a solution:\n",
    "# \n",
    "# $ python3 solution.py\n",
    "# Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# Enter count: 4\n",
    "# Enter position: 3\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
    "# Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n",
    "# The answer to the assignment for this execution is \"Anayah\".\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Murdo.html'\n",
    "repeatprocess = 0\n",
    "while repeatprocess < 7:\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # retrieve all the anchor tags from the page\n",
    "    tags = soup('a')    # find the '<a' in the html page code\n",
    "    countresults = 0\n",
    "    for tag in tags:         # this loop finds the 'href' that is the actually link\n",
    "        countresults = countresults + 1\n",
    "        if countresults == 18:\n",
    "            repeatprocess = repeatprocess + 1\n",
    "            url = str(tag.get('href', None))\n",
    "            print(url)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Attr: yes\n"
     ]
    }
   ],
   "source": [
    "# Web Services and XML\n",
    "# # 13.1 Data on the Web\n",
    "# the way information is send through web in a way the receiver can understand it\n",
    "# this is called serialize and de-serialize...\n",
    "# formats we will cover is XML and JSON\n",
    "\n",
    "# 13.2 XML\n",
    "# eXtensible Markup Language\n",
    "# share structured data\n",
    "# uses tags to define start and end\n",
    "# got attributes about tags\n",
    "# text content for the content itself\n",
    "# it is structured in nodes, like a tree roots (paths)\n",
    "\n",
    "# 13.3 XML Schema\n",
    "# there are contracts between aplications to define good practices... validation = confirm if the document is well constructed\n",
    "# XSD\n",
    "# XSD Data Types: string, date, datetime, decimal and integer\n",
    "# Date/Time Format: 2023-11-03T07:10Z (Z is the Timezone)\n",
    "\n",
    "# 13.4 Parsing XML\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "data = '''<person>\n",
    "<name>Chuck</name>\n",
    "<phone type=\"intl\">\n",
    "    +1 734 303 4456\n",
    "    </phone>\n",
    "    <email hide=\"yes\"/>\n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data)    # command to see what u got in data as a long string, as a \"tree\" of XML\n",
    "print('Name:', tree.find('name').text)     # this brings the text taged as 'name'\n",
    "print('Attr:', tree.find('email').get('hide'))   # this brings the attribute 'hide' int the tag email\n",
    "# u can use .findall('user/users') to create a list of \"trees\"\n",
    "tree.find('users/user') # this one, brings a tag inside another tag!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2486\n",
      "note\n",
      "comments\n"
     ]
    }
   ],
   "source": [
    "# Extracting Data from XML\n",
    "# \n",
    "# In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "# \n",
    "# We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "# \n",
    "# Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "# Actual data: http://py4e-data.dr-chuck.net/comments_1920407.xml (Sum ends with 86)\n",
    "# You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "# Data Format and Approach\n",
    "# The data consists of a number of names and comment counts in XML as follows:\n",
    "# \n",
    "# <comment>\n",
    "#   <name>Matthias</name>\n",
    "#   <count>97</count>\n",
    "# </comment>\n",
    "# You are to look through all the <comment> tags and find the <count> values sum the numbers. The closest sample code that shows how to parse XML is geoxml.py. But since the nesting of the elements in our data is different than the data we are parsing in that sample code you will have to make real changes to the code.\n",
    "# To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:\n",
    "# \n",
    "# counts = tree.findall('.//count')\n",
    "# Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details. You could also work from the top of the XML down to the comments node and then loop through the child nodes of the comments node.\n",
    "# Sample Execution\n",
    "# \n",
    "# $ python3 solution.py\n",
    "# Enter location: http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "# Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "# Retrieved 4189 characters\n",
    "# Count: 50\n",
    "# Sum: 2...\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "sumcount = 0\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_1920407.xml'\n",
    "urlhandle = urllib.request.urlopen(url)\n",
    "xml = urlhandle.read().decode()\n",
    "tree = ET.fromstring(xml)\n",
    "listofcomments = tree.findall('./comments/comment')    # u need to pay attention to each tag node to \"open\" it like directories in your windows explorer...\n",
    "for counts in listofcomments:\n",
    "    count = counts.find('count').text\n",
    "    sumcount = sumcount + int(count)\n",
    "print(sumcount)\n",
    "\n",
    "# checking nodes inside the main node:\n",
    "for nodes in tree:\n",
    "    print(nodes.tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Chuck\n",
      "Hide:  yes\n"
     ]
    }
   ],
   "source": [
    "# 13.5 JSON and the REST Architecture\n",
    "# JavaScript Objetct Notation (JSON)\n",
    "\n",
    "import json\n",
    "data = '''{\n",
    "    \"name\" : \"Chuck\",\n",
    "    \"phone\" : {\n",
    "        \"type\" : \"intl\",\n",
    "        \"number\" : \"+1 734 303 4456\"\n",
    "        },\n",
    "        \"email\" : {\n",
    "            \"hide\" : \"yes\"\n",
    "            }\n",
    "}'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('Name: ', info[\"name\"])\n",
    "print('Hide: ', info[\"email\"][\"hide\"])\n",
    "\n",
    "# JSON is simpler and has some differences from XML but is friendlier to programming languages\n",
    "# it uses similar structures to Python dictionaries and lists, representing data as nested \"lists\" and \"dictionaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.6 Service Oriented Approach\n",
    "# Analogy: when you are booking a flight and got a car reservation in the same site...\n",
    "# It brings information from another system, from the car rental company to your screen in the same site..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.7 Using Application Programming Interfaces (API)\n",
    "# example used: google maps API\n",
    "# u need to understand the syntax they use, them adapt your code to their information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.8 Securing API Requests\n",
    "# API Security and Rate Limiting\n",
    "# this is not \"free\"... google and other platforms, limit the number of daily requests...\n",
    "# other examples: Twitter API requests login to let u take information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Jaskaran', 'count': 100}, {'name': 'Meleia', 'count': 99}, {'name': 'Liva', 'count': 98}, {'name': 'Taira', 'count': 90}, {'name': 'Ramone', 'count': 89}, {'name': 'Miyah', 'count': 80}, {'name': 'Bethlin', 'count': 79}, {'name': 'Skyla', 'count': 78}, {'name': 'Maleetah', 'count': 75}, {'name': 'Abia', 'count': 72}, {'name': 'Asrar', 'count': 72}, {'name': 'Jaosha', 'count': 70}, {'name': 'Aryan', 'count': 70}, {'name': 'Mohamed', 'count': 69}, {'name': 'Ailsa', 'count': 68}, {'name': 'Nikita', 'count': 68}, {'name': 'Aysha', 'count': 68}, {'name': 'Michat', 'count': 66}, {'name': 'Findlie', 'count': 65}, {'name': 'Bryce', 'count': 63}, {'name': 'Burak', 'count': 61}, {'name': 'Tony', 'count': 60}, {'name': 'Tokunbo', 'count': 59}, {'name': 'Anja', 'count': 59}, {'name': 'Benedict', 'count': 59}, {'name': 'Philippos', 'count': 58}, {'name': 'Ailiegh', 'count': 57}, {'name': 'Morgyn', 'count': 56}, {'name': 'Bilal', 'count': 55}, {'name': 'Peirce', 'count': 53}, {'name': 'Aldred', 'count': 50}, {'name': 'Arfa', 'count': 47}, {'name': 'Pierce', 'count': 46}, {'name': 'Aanya', 'count': 44}, {'name': 'Hawaa', 'count': 41}, {'name': 'Kairn', 'count': 39}, {'name': 'Keetza', 'count': 33}, {'name': 'Francisco', 'count': 32}, {'name': 'Rylee', 'count': 28}, {'name': 'Alissa', 'count': 22}, {'name': 'Aron', 'count': 21}, {'name': 'Tait', 'count': 21}, {'name': 'Vhairi', 'count': 19}, {'name': 'Kaisha', 'count': 12}, {'name': 'Alx', 'count': 12}, {'name': 'Cally', 'count': 10}, {'name': 'Zunera', 'count': 10}, {'name': 'Deni', 'count': 8}, {'name': 'Kashish', 'count': 5}, {'name': 'Briana', 'count': 2}]\n",
      "2618\n"
     ]
    }
   ],
   "source": [
    "# Extracting Data from JSON\n",
    "# \n",
    "# In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\n",
    "# We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "# \n",
    "# Sample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)\n",
    "# Actual data: http://py4e-data.dr-chuck.net/comments_1920408.json (Sum ends with 18)\n",
    "# You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "# Data Format\n",
    "# The data consists of a number of names and comment counts in JSON as follows:\n",
    "# \n",
    "# {\n",
    "#   comments: [\n",
    "#     {\n",
    "#       name: \"Matthias\"\n",
    "#       count: 97\n",
    "    # },\n",
    "    # {\n",
    "#       name: \"Geomer\"\n",
    "#       count: 97\n",
    "#     }\n",
    "#     ...\n",
    "#   ]\n",
    "# }\n",
    "# The closest sample code that shows how to parse JSON and extract a list is json2.py. You might also want to look at geoxml.py to see how to prompt for a URL and retrieve data from a URL.\n",
    "\n",
    "# Sample Execution\n",
    "# \n",
    "# $ python3 solution.py\n",
    "# Enter location: http://py4e-data.dr-chuck.net/comments_42.json\n",
    "# Retrieving http://py4e-data.dr-chuck.net/comments_42.json\n",
    "# Retrieved 2733 characters\n",
    "# Count: 50\n",
    "# Sum: 2...\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "count = 0\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_1920408.json'\n",
    "openurl = urllib.request.urlopen(url).read().decode()\n",
    "info = json.loads(openurl)\n",
    "print(info[\"comments\"])\n",
    "for infos in info[\"comments\"]:\n",
    "    count = count + infos['count']\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=Jawaharlal+Nehru+University&key=42\n",
      "Retrieved 2529 characters\n",
      "ChIJt4MB6sAdDTkR52Hwx5rR2O8\n"
     ]
    }
   ],
   "source": [
    "# Calling a JSON API\n",
    "# \n",
    "# In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geojson.py. The program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\n",
    "# API End Points\n",
    "# \n",
    "# To complete this assignment, you should use this API endpoint that has a static subset of the Google Data:\n",
    "# \n",
    "# http://py4e-data.dr-chuck.net/json?\n",
    "# This API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get \"No address...\" response.\n",
    "# To call the API, you need to include a key= parameter and provide the address that you are requesting as the address= parameter that is properly URL encoded using the urllib.parse.urlencode() function as shown in http://www.py4e.com/code3/geojson.py\n",
    "# \n",
    "# Make sure to check that your code is using the API endpoint as shown above. You will get different results from the geojson and json endpoints so make sure you are using the same end point as this autograder is using.\n",
    "# \n",
    "# Test Data / Sample Execution\n",
    "# \n",
    "# You can test to see if your program is working with a location of \"South Federal University\" which will have a place_id of \"ChIJNeHD4p-540AR2Q0_ZjwmKJ8\".\n",
    "# \n",
    "# $ python3 solution.py\n",
    "# Enter location: South Federal University\n",
    "# Retrieving http://...\n",
    "# Retrieved 6052 characters\n",
    "# Place id ChIJNeHD4p-540AR2Q0_ZjwmKJ8\n",
    "# Turn In\n",
    "# \n",
    "# Please run your program to find the place_id for this location:\n",
    "# \n",
    "# Jawaharlal Nehru University\n",
    "# Make sure to enter the name and case exactly as above and enter the place_id and your Python code below. Hint: The first seven characters of the place_id are \"ChIJt4M ...\"\n",
    "# Make sure to retreive the data from the URL specified above and not the normal Google API. Your program should work with the Google API - but the place_id may not match for this assignment.\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "api_key = 42\n",
    "serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    address_dict = dict()\n",
    "    address_dict['address'] = address\n",
    "    address_dict['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(address_dict)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    place_id = js['results'][0]['place_id']\n",
    "    print(place_id)\n",
    "    \n",
    "    # first place_id from the JSON\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
